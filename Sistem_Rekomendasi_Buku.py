# -*- coding: utf-8 -*-
"""Recommendation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l65_bkowfgy-X_Rlh9wF2iIxn5BZB5a8

# **1. Import Library**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow import keras

from sklearn.metrics import mean_squared_error
from sklearn.metrics import precision_recall_fscore_support

"""# **2. Data Loading**

Untuk dapat mengambil dataset dari Kaggle, perlu dilakukan konfigurasi kredensial API Kaggle di Google Colab. Kaggle menyediakan API yang memungkinkan akses langsung ke dataset mereka tanpa perlu mengunduhnya secara manual.

Proses ini dilakukan dengan mengunggah file kaggle.json yang berisi kredensial API yang diunduh dari halaman akun Kaggle.
"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

# Hapus semua folder yang berpotensi bentrok
!rm -rf "book-recommendation-dataset.zip"
!rm -rf "Books.csv"
!rm -rf "Ratings.csv"
!rm -rf "Users.csv"

# Setup kredensial Kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset Anemia Dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# Unzip file dataset
!unzip book-recommendation-dataset.zip

# Hapus semua folder yang tidak dibutuhkan
!rm -rf "classicRec.png"
!rm -rf "recsys_taxonomy2.png"
!rm -rf "DeepRec.png"

"""Dengan kredensial yang telah disiapkan, kode *!kaggle datasets download -d arashnic/book-recommendation-dataset* digunakan untuk mengunduh dataset yang berjudul "book-recommendation-dataset" dari Kaggle.

Setelah itu, file yang diunduh (dalam format ZIP) diekstrak menggunakan perintah *!unzip book-recommendation-dataset.zip*. Ekstraksi ini bertujuan untuk mengakses file data dalam format CSV yang ada di dalamnya.

Setelah itu, kita perlu mengubah dataset yang diunduh dalam format CSV ke dalam variabel DataFrame.
"""

# Membaca dataset
books = pd.read_csv("Books.csv")
users = pd.read_csv("Users.csv")
ratings = pd.read_csv("Ratings.csv")

"""# **3. Data Understanding**

## **Books.csv**

Pertama kita lihat terlebih dahulu struktur books.csv dengan fungsi `head()` dan `info()`
"""

books.head()

# Menampilkan informasi tentang dataset
books.info()

"""Berdasarkan hasil pemanggilan fungsi books.info(), dataset Books.csv terdiri dari 271.360 entri dengan 8 kolom. Detail tiap kolom:
- ISBN: Nomor ISBN yang digunakan untuk mengidentifikasi buku.
- Book-Title: Judul buku.
- Book-Author: Penulis buku (hanya penulis pertama jika ada lebih dari satu).
- Year-Of-Publication: Tahun terbit buku.
- Publisher: Penerbit buku.
- Image-URL-S: URL gambar sampul buku dalam ukuran kecil.
- Image-URL-M: URL gambar sampul buku dalam ukuran sedang.
- Image-URL-L: URL gambar sampul buku dalam ukuran besar.

Untuk keperluan analisis dan pembuatan sistem rekomendasi, tipe data pada kolom `Year-Of-Publication` diubah menjadi `int64` agar dapat digunakan secara numerik. Selain itu, kolom `Image-URL-S`, `Image-URL-M`, dan `Image-URL-L` dihapus karena hanya berisi URL gambar dan tidak relevan dalam konteks content-based filtering maupun collaborative filtering.

Selanjutnya kita memeriksa apakah ada missing value.
"""

# Memeriksa missing value
books.isnull().sum()

"""Terdapat 3 missing value pada `Image-URL-L`, tetap dapat diabaikan karena nantinya kolom akan di-drop. Selain itu ada juga missing value pada `Book-Author` dan `Publisher` yang memerlukan penanganan imputasi karena kolom ini digunakan pada pembuatan sistem rekomendasi

Cek di data ke berapa nilai null untuk memudahkan penanganan missing value atau imputasi data
"""

books.loc[books['Book-Author'].isnull(),:]

books.loc[books['Publisher'].isnull(),:]

"""Setelah itu, dilakukan pengecekan pada kolom `Year-Of-Publication`"""

# Mendapatkan nilai unik dan mengurutkannya sebagai string
uniqe_years = sorted(books['Year-Of-Publication'].unique(), key=str)

# Menentukan jumlah item per baris
items_per_row = 20

# Menampilkan tahun yang sudah diurutkan dalam format grid
for i in range(0, len(uniqe_years), items_per_row):
    print(" ".join(str(year) for year in uniqe_years[i:i+items_per_row]))

"""Dari hasil pemeriksaan pada kolom Year-Of-Publication, ditemukan dua entri yang tidak sesuai dengan format yang diharapkan, di mana kolom ini berisi nama penerbit seperti "DK Publishing Inc" dan "Gallimard", padahal kolom tersebut seharusnya hanya berisi tahun penerbitan buku"""

books.loc[books['Year-Of-Publication'] == 'DK Publishing Inc',:]

books.loc[books['Year-Of-Publication'] == 'Gallimard',:]

"""Setelah pengecekan, terdapat kesalahan input yang menyebabkan data pada beberapa kolom tergeser. Tiga entri pada kolom `Book-Author` berisi gabungan antara judul buku dan nama penulis. Perlu dilakukan pemisahan data dan perbaikan nilai pada kolom `Year-Of-Publication` agar formatnya konsisten dan valid untuk analisis.

Selanjutnya, kita memeriksa apakah terdapat karakter non-alfanumerik pada kolom ISBN dalam data rating.
"""

# Inisialisasi flag dan daftar kosong
flag = 0
k = []

# Menentukan pola ekspresi reguler untuk karakter non-alfanumerik
pattern = "[^A-Za-z0-9]"

# Mengecek setiap nilai ISBN dalam data books
for isbn in ratings['ISBN']:
    # Mencari karakter yang tidak sesuai dengan pola alfanumerik
    result = re.search(pattern, isbn)

    # Jika ditemukan karakter non-alfanumerik
    if result:
        flag = 1

# Jika flag bernilai 1, berarti ditemukan karakter non-alfanumerik
if flag == 1:
    print("False")
else:
    print("True")

"""Dari hasil pengecekan, menggunakan flag, ditemukan bahwa ada karakter non-alfanumerik yang tidak sesuai dengan format yang diharapkan, sehingga perlu dilakukan perbaikan pada data tersebut.

Setelah itu, kita periksa apakah ada data yang duplikat
"""

# Memeriksa duplikasi data
jumlah_duplikat = books.duplicated().sum()
print(f"Jumlah baris duplikat: {jumlah_duplikat}")

"""Pada dataset ini, tidak ditemukan duplikat.

## **Users.csv**

Pertama kita lihat terlebih dahulu struktur users.csv dengan fungsi `head()` dan `info()`
"""

users.head()

# Menampilkan informasi tentang dataset
users.info()

"""Dataset Users memiliki kolom Location yang menggabungkan informasi kota, negara bagian, dan negara dalam satu kolom seperti "nyc, new york, usa". Meskipun kolom ini tidak digunakan dalam model rekomendasi, pemisahan menjadi City, State, dan Country penting untuk menjaga konsistensi data dan memudahkan pengelolaan di masa depan.

Berdasarkan hasil pemanggilan fungsi users.info(), dataset Books.csv terdiri dari 278.858 entri dengan 3 kolom. Detail tiap kolom:
- User-ID: ID pengguna yang telah dianonimkan.
- Location: Lokasi pengguna.
- Age: Umur pengguna.

Dataset terdiri dari kolom User-ID dengan tipe int64, Location dengan tipe data object, dan Age dengan tipe float64, yang sudah sesuai dengan jenis data masing-masing.

Selanjutnya kita memeriksa apakah ada missing value.
"""

# Memeriksa missing value
users.isnull().sum()

"""Ditemukan bahwa kolom Age memiliki 110,762 entri yang kosong. Meskipun kolom Age tidak digunakan dalam model rekomendasi berbasis Content-Based Filtering dan Collaborative Filtering, penting untuk menangani nilai yang hilang pada kolom ini untuk menjaga konsistensi data

Selanjutnya, dilakukan pengecekan lebih lanjut pada kolom Age
"""

# Mendapatkan nilai unik umur dan mengurutkannya
unique_ages = sorted(users['Age'].unique())

# Menentukan jumlah item per baris
items_per_row = 20

# Menampilkan usia yang sudah diurutkan dalam format grid dengan 20 nilai per baris
for i in range(0, len(unique_ages), items_per_row):
    print(" ".join(str(age) for age in unique_ages[i:i+items_per_row]))

"""Berdasarkan hasil pengecekan data pada kolom Age, dapat dilihat bahwa bahwa terdapat nilai NaN (missing values) dalam data usia. Setelah disortir, terdapat berbagai rentang usia yang tercatat dalam dataset. Rentang usia ini bervariasi dari 0 hingga 244. Ada beberapa nilai usia yang lebih tinggi (lebih dari 80), yang bisa jadi merupakan entri yang tidak valid atau mungkin hasil kesalahan input

Setelah itu, kita periksa apakah ada data yang duplikat.
"""

# Memeriksa duplikasi data
jumlah_duplikat = users.duplicated().sum()
print(f"Jumlah baris duplikat: {jumlah_duplikat}")

"""Tidak ditemukan data duplikat pada users.csv

## **Ratings.csv**

Pertama kita lihat terlebih dahulu struktur ratings.csv dengan fungsi `head()` dan `info()`
"""

ratings.head()

# Menampilkan informasi tentang dataset
ratings.info()

"""Berdasarkan hasil pemanggilan fungsi users.info(), dataset Books.csv terdiri dari 1.149.780 entri dengan 3 kolom. Detail tiap kolom:
- User-ID: ID pengguna yang memberikan rating pada buku.
- ISBN: ISBN buku yang diberi rating.
- Book-Rating: Rating yang diberikan oleh pengguna, dengan rentang nilai 1-10 (nilai 0 menandakan rating implisit).

Tipe data User-ID dan Book-Rating sudah int64 sesuai kebutuhan, sedangkan ISBN bertipe object yang tepat karena berisi data alfanumerik.

Selanjutnya kita memeriksa apakah ada missing value.
"""

# Memeriksa missing value
ratings.isnull().sum()

"""Tidak ditemukan missing value pada dataset ratings.csv

Selanjutnya, kita memeriksa apakah terdapat karakter non-alfanumerik pada kolom ISBN dalam data rating.
"""

# Inisialisasi flag dan daftar kosong
flag = 0
k = []

# Menentukan pola ekspresi reguler untuk karakter non-alfanumerik
pattern = "[^A-Za-z0-9]"

# Mengecek setiap nilai ISBN dalam data ratings
for isbn in ratings['ISBN']:
    # Mencari karakter yang tidak sesuai dengan pola alfanumerik
    result = re.search(pattern, isbn)

    # Jika ditemukan karakter non-alfanumerik
    if result:
        flag = 1

# Jika flag bernilai 1, berarti ditemukan karakter non-alfanumerik
if flag == 1:
    print("False")
else:
    print("True")

"""Dari hasil pengecekan, menggunakan flag, ditemukan bahwa ada karakter non-alfanumerik yang tidak sesuai dengan format yang diharapkan, sehingga perlu dilakukan perbaikan pada data tersebut.

Selanjutnya, dilakukan pengecekan deskripsi statistik data dengan fitur `describe()`.
"""

# Mengatur pandas untuk menampilkan angka dalam format float standar
pd.set_option('display.float_format', '{:,.0f}'.format)

# Menampilkan deskripsi statistik dari DataFrame 'ratings'
ratings.describe()

"""Kolom Book-Rating memiliki nilai antara 0 sampai 10, dengan 0 menandakan tidak ada rating (interaksi implisit). Untuk meningkatkan kualitas model rekomendasi, data dengan rating 0 sebaiknya dihapus agar hanya interaksi nyata yang digunakan dalam proses.

Setelah itu, kita periksa apakah ada data yang duplikat.
"""

# Memeriksa duplikasi data
jumlah_duplikat = ratings.duplicated().sum()
print(f"Jumlah baris duplikat: {jumlah_duplikat}")

"""Tidak ada duplikasi data pada ratings.csv

# **4. Exploratory Data Analysis**

Selanjutnya, kita akan melakukan proses analisis data dengan teknik Univariate Analysis.

## **Univariate Analysis**

Pertama, kita akan melihat distribusi jumlah buku berdasarkan penulis
"""

# Mengatur ukuran figure untuk plot
plt.figure(figsize=(15, 6))

# Menampilkan jumlah buku berdasarkan penulis, mengurutkan berdasarkan jumlah buku terbanyak
sns.countplot(y="Book-Author", data=books, order=books['Book-Author'].value_counts().index[:10])

# Menambahkan judul dan label pada plot
plt.title("10 Penulis dengan Jumlah Buku Terbanyak")
plt.xlabel("Jumlah Buku")
plt.ylabel("Penulis")

# Menampilkan plot
plt.show()

"""Grafik menunjukkan 10 penulis dengan jumlah buku terbanyak dalam dataset. Agatha Christie menduduki posisi teratas dengan lebih dari 600 judul buku, diikuti oleh William Shakespeare dan Stephen King dengan jumlah buku mendekati 500-600 judul. Penulis lain seperti Ann M. Martin, Carolyn Keene, dan Isaac Asimov juga memiliki jumlah buku yang signifikan (sekitar 300-400 judul). Hal ini menunjukkan bahwa karya-karya dari penulis produktif ini sangat dominan dalam dataset, yang dapat memengaruhi rekomendasi buku, terutama dalam metode content-based filtering dan collaborative filtering karena popularitas dan volume karya mereka yang besar.

Selanjutnya, kita akan melihat distribusi jumlah buku berdasarkan penerbit
"""

# Mengatur ukuran figure untuk plot
plt.figure(figsize=(15, 6))

# Menampilkan jumlah buku berdasarkan penerbit, mengurutkan berdasarkan jumlah buku terbanyak
sns.countplot(y="Publisher", data=books, order=books['Publisher'].value_counts().index[:10])

# Menambahkan judul dan label pada plot
plt.title("10 Penerbit dengan Jumlah Buku Terbanyak")
plt.xlabel("Jumlah Buku")
plt.ylabel("Penerbit")

# Menampilkan plot
plt.show()

"""Harlequin mendominasi jumlah buku jauh lebih banyak dibanding penerbit lain seperti Silhouette, Pocket, dan Ballantine Books yang jumlahnya relatif seimbang namun jauh lebih sedikit. Dominasi ini berpotensi memengaruhi fokus analisis dan sistem rekomendasi, terutama pada content-based dan collaborative filtering.

Selanjutnya, dilakukan visualisasi distribusi usia pengguna
"""

# Menentukan ukuran figure untuk plot
plt.figure(figsize=(10, 6))

# Membuat histogram untuk kolom 'Age' tanpa menghapus nilai yang hilang
plt.hist(users['Age'], bins=20, edgecolor='black')

# Menambahkan judul dan label pada plot
plt.title('Distribusi Umur Pengguna')
plt.xlabel('Umur')
plt.ylabel('Jumlah User')

# Menambahkan grid untuk memudahkan pembacaan
plt.grid(True)

# Menampilkan plot
plt.show()

"""Distribusi umur menunjukkan data tidak valid di rentang dekat 0 dan di atas 100 tahun. Data invalid ini dapat memengaruhi akurasi rekomendasi jika umur dipakai, sehingga perlu penanganan dengan imputasi menggunakan nilai rata-rata.

Dilanjutkan dengan visualisasi distribusi rating
"""

# Menentukan ukuran figure untuk plot
plt.figure(figsize=(8, 6))

# Membuat countplot untuk distribusi rating buku
sns.countplot(x="Book-Rating", data=ratings)

# Menambahkan judul dan label pada plot
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating Buku')
plt.ylabel('Jumlah Rating')

# Menampilkan plot
plt.show()

"""Distribusi rating didominasi oleh nilai 0, menandakan banyak interaksi implisit. Rating 0 perlu dihapus agar fokus pada preferensi pengguna nyata (1–10), menghasilkan dataset lebih kecil tapi lebih relevan untuk collaborative filtering.

Sebelum analisis lebih lanjut, kita harus menggabungkan dataset ratings dan books terlebih dahulu dengan membentuk DataFrame baru khusus untuk analisis saja
"""

dataset_analysis = pd.merge(ratings, books, on='ISBN')

"""Kita akan memahami hubungan popularitas pengguna terhadap buku tertentu."""

# Mengelompokkan berdasarkan Book-Title dan menghitung jumlah rating per buku
top_books = dataset_analysis['Book-Title'].value_counts().head(10)

# Mengatur ukuran figure untuk plot
plt.figure(figsize=(15, 8))

# Membuat plot batang horizontal untuk 10 buku yang paling banyak diberi rating
sns.barplot(x=top_books.values, y=top_books.index)

# Menambahkan judul dan label
plt.title("10 Buku yang Paling Banyak Diberikan Rating")
plt.xlabel("Jumlah Rating")
plt.ylabel("Judul Buku")

# Menampilkan plot
plt.show()

"""Visualisasi Buku yang Paling Banyak Diberikan Rating mengidentifikasi 'Wild Animus' sebagai buku yang menerima rating terbanyak secara signifikan, diikuti oleh 'The Lovely Bones: A Novel'. Buku-buku dengan jumlah rating tinggi ini mengindikasikan popularitas yang besar di kalangan pengguna dan dapat menjadi rekomendasi yang kuat dalam sistem."""

# Mengambil 10 pengguna teratas yang memberikan rating terbanyak
top_10_users = dataset_analysis['User-ID'].value_counts().head(10)

# Mengatur ukuran figure untuk plot
plt.figure(figsize=(15, 8))

# Membuat plot batang horizontal untuk 10 pengguna teratas berdasarkan jumlah buku yang dinilai terbanyak
sns.barplot(x=top_10_users.values, y=top_10_users.index, orient='h')

# Menambahkan judul dan label
plt.title("10 Pengguna Teratas yang Memberikan Rating Terbanyak")
plt.xlabel("Jumlah Buku yang Dinilai")
plt.ylabel("User-ID")

# Menampilkan plot
plt.show()

"""Pengguna ID 11676 memberikan rating paling banyak, menyediakan data preferensi yang kaya untuk collaborative filtering. Data ini membantu mengenali pola kesamaan antar pengguna dan meningkatkan rekomendasi.

# **5. Data Preparation**

## **Penghapusan Kolom yang Tidak Diperlukan (_dropping coloumns_)**
"""

# Drop URL columns
books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

"""## **Penanganan Missing Value**

Berdasarkan hasil pengecekan pada data understanding, terdapat missing value pada dataset books dan users yang perlu penanganan.

**Books**
"""

# Menetapkan nilai 'Other' pada kolom 'Book-Author' di baris 187689 dan 118033
books.loc[187689, 'Book-Author'] = 'Other'
books.loc[118033, 'Book-Author'] = 'Other'

# Menetapkan nilai 'Other' pada kolom 'Publisher' di baris 128890 dan 129037
books.loc[128890, 'Publisher'] = 'Other'
books.loc[129037, 'Publisher'] = 'Other'

"""**Users**"""

# Menghitung rata-rata usia (mean) dari kolom 'Age', mengabaikan nilai NaN
mean_age = users['Age'].mean()

# Mengganti nilai NaN atau data yang tidak valid (misalnya nilai negatif) dengan rata-rata usia
users['Age'] = users['Age'].apply(lambda x: mean_age if pd.isna(x) else x)

"""Penanganan missing value dilakukan dengan imputasi: nilai 'Other' untuk kolom `Book-Author` dan `Publisher` pada dataset books, serta imputasi rata-rata untuk kolom 'Age' pada dataset users. Langkah ini penting agar data siap digunakan dalam sistem rekomendasi.

## **Standarisasi Format**

Pada tahap ini dilakukan standarisasi format diterapkan pada kolom ISBN yang terdapat baik dalam dataset books maupun ratings. Proses ini melibatkan pengubahan seluruh karakter dalam kolom 'ISBN' menjadi huruf kapital menggunakan fungsi `.str.upper()` pada kedua dataset.
"""

books['ISBN'] = books['ISBN'].str.upper()

ratings['ISBN'] = ratings['ISBN'].str.upper()

"""## **Penanganan Invalid Data**

Pada Data Understanding ditemukan banyak invalid data yang perlu diperbaiki. Berikut adalah penanganannya.

**Mengatasi Kesalahan Data yang Tergeser**
"""

# Memperbarui informasi di baris 209538
books.loc[209538, 'Publisher'] = 'DK Publishing Inc'
books.loc[209538, 'Year-Of-Publication'] = 2000
books.loc[209538, 'Book-Title'] = 'DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)'
books.loc[209538, 'Book-Author'] = 'Michael Teitelbaum'

# Memperbarui informasi di baris 221678
books.loc[221678, 'Publisher'] = 'DK Publishing Inc'
books.loc[221678, 'Year-Of-Publication'] = 2000
books.loc[221678, 'Book-Title'] = 'DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)'
books.loc[221678, 'Book-Author'] = 'James Buckley'

# Memperbarui informasi di baris 220731
books.loc[220731, 'Publisher'] = 'Gallimard'
books.loc[220731, 'Year-Of-Publication'] = 2003
books.loc[220731, 'Book-Title'] = 'Peuple du ciel - Suivi de Les bergers'
books.loc[220731, 'Book-Author'] = 'Jean-Marie Gustave Le Clézio'

# Menampilkan hasil pembaruan
books.loc[[209538, 221678, 220731]]

"""Dilakukan pembaruan data secara manual (explicit value replacement) pada baris tertentu di dataset books untuk memperbaiki informasi yang salah, seperti judul, penulis, penerbit, dan tahun publikasi. Langkah ini penting agar data akurat dan dapat diandalkan dalam analisis dan sistem rekomendasi.

**Mengubah Invalid Years pada "Years-of-Publication"**

Tipe data Year-Of-Publication diubah menjadi tipe integer untuk memastikan bahwa tahun publikasi dapat diolah dengan benar dalam analisis numerik
"""

# Mengonversi 'Year-Of-Publication' menjadi tipe data integer
books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)

# Mengganti tahun yang tidak valid (tahun 0 atau lebih besar dari 2025) dengan mode tahun
mode_year = books['Year-Of-Publication'].mode()[0]

# Terapkan perubahan pada kolom 'Year-Of-Publication'
books['Year-Of-Publication'] = books['Year-Of-Publication'].apply(
    lambda x: mode_year if x == 0 or x > 2025 else x
)

"""Dilakukan juga imputasi nilai tidak valid (misalnya 0 atau >2025) dengan nilai **mode**. Langkah ini dilakukan menghindari kesalahan interpretasi.

**Menghapus Karakter Tidak Valid pada Kolom ISBN**

Selanjutnya, dilakukan penghapusan karakter tidak valid pada kolom 'ISBN' untuk memastikan hanya karakter alfanumerik yang tersisa
"""

# Fungsi untuk membersihkan ISBN dan menghapus karakter selain huruf dan angka
def clean_isbn(isbn):
    # Menghapus semua karakter selain huruf dan angka
    return re.sub(r'[^A-Za-z0-9]', '', isbn)

# Mengaplikasikan pembersihan ke kolom ISBN pada dataset books
books['ISBN'] = books['ISBN'].apply(lambda x: clean_isbn(x))

# Mengecek apakah ISBN sekarang valid (hanya terdiri dari huruf dan angka)
flag = 0
for x in books['ISBN']:
    if re.search("[^A-Za-z0-9]", x):
        flag = 1
        break

if flag == 1:
    print("False: ISBN contains invalid characters after cleaning")
else:
    print("True: All ISBNs are valid")

# Mengaplikasikan pembersihan ke kolom ISBN pada dataset ratings
ratings['ISBN'] = ratings['ISBN'].apply(lambda x: clean_isbn(x))

# Mengecek apakah ISBN sekarang valid (hanya terdiri dari huruf dan angka)
flag = 0
for x in ratings['ISBN']:
    if re.search("[^A-Za-z0-9]", x):
        flag = 1
        break

if flag == 1:
    print("False: ISBN contains invalid characters after cleaning")
else:
    print("True: All ISBNs are valid")

"""Setelah dilakukan pembersihan, dilakukan pengecekan ulang. Hasil dari pengecekan ulang menunjukkan data ISBN sudah valid (terdiri data data alfanumerik). Langkah ini penting untuk menjaga format standar ISBN, memfasilitasi pencocokan antar dataset, dan meningkatkan akurasi identifikasi buku dalam sistem rekomendasi.

**Mengubah Nilai Invalid Age**

Selanjutnya, penanganan nilai invalid untuk kolom 'Age', digunakan teknik imputasi dengan nilai rata-rata atau mean imputation untuk mengganti nilai-nilai usia yang di luar batas wajar (kurang dari 10 atau lebih dari 80 tahun).
"""

# Mengganti nilai yang lebih besar dari 80 atau lebih kecil dari 10 dengan rata-rata usia
users['Age'] = users['Age'].apply(lambda x: mean_age if x < 10 or x > 80 else x)

"""**Memisahkan Lokasi (City, State, Country)**

Selanjutnya, dilakukan ekstraksi dan pemisahan data pada kolom 'Location' menjadi kolom 'City', 'State', dan 'Country'. Langkah ini bertujuan untuk memperoleh informasi geografis yang lebih terstruktur guna mendukung analisis preferensi pengguna berdasarkan lokasi.
"""

# Membagi data lokasi menjadi kota, negara bagian, dan negara
location_split = users.Location.str.split(', ')

# Inisialisasi list untuk menyimpan hasil dan hitung entri yang tidak valid
kota = []
negara_bagian = []
negara = []
count_no_state = 0
count_no_country = 0

# Iterasi untuk memproses setiap entri lokasi
for item in location_split:
    # Proses kota
    if item[0] in (' ', '', 'n/a', ','):
        kota.append('other')
    else:
        kota.append(item[0].lower())

    # Proses negara bagian dan negara
    if len(item) < 2:
        negara_bagian.append('other')
        negara.append('other')
        count_no_state += 1
        count_no_country += 1
    else:
        if item[1] in (' ', '', 'n/a', ','):
            negara_bagian.append('other')
            count_no_state += 1
        else:
            negara_bagian.append(item[1].lower())

        if len(item) < 3:
            negara.append('other')
            count_no_country += 1
        else:
            if item[2] in (' ', '', 'n/a', ','):
                negara.append('other')
                count_no_country += 1
            else:
                negara.append(item[2].lower())

# Menghapus kolom 'Location' dari DataFrame
users = users.drop('Location', axis=1)

# Menangani kasus kota yang diikuti oleh negara bagian, dan hanya mengambil kota
kota_temp = [ent.split('/')[0] for ent in kota]

# Membuat DataFrame baru untuk Kota, Negara Bagian, dan Negara
df_kota = pd.DataFrame(kota_temp, columns=['City'])
df_negara_bagian = pd.DataFrame(negara_bagian, columns=['State'])
df_negara = pd.DataFrame(negara, columns=['Country'])

# Menggabungkan DataFrame baru ke dalam DataFrame 'users'
users = pd.concat([users, df_kota], axis=1)
users = pd.concat([users, df_negara_bagian], axis=1)
users = pd.concat([users, df_negara], axis=1)

# Mencetak jumlah entri negara dan negara bagian yang tidak memiliki nilai
print(count_no_country)
print(count_no_state)

users.head()

"""Setelah penanganan dapat dilihat bahwa kolom `Location` sudah diubah menjadi `City`, `State`, dan `Country`

**Menghapus Rating dengan Nilai 0**

Selanjutnya, dilakukan pembersihan data pada dataset ratings dengan melakukan data filtering pada kolom `Book-Rating` dengan menghapus nilai 0, karena dianggap sebagai interaksi implisit yang dapat menimbulkan noise. Langkah ini membantu memfokuskan analisis pada preferensi pengguna yang jelas dan eksplisit.
"""

ratings = ratings[ratings['Book-Rating'] != 0]
ratings = ratings.reset_index(drop = True)

"""## **Menghapus Data Duplikat**

Selanjutnya dilakukan penanganan untuk duplikasi data. Sebelum itu, dilakukan pengecekan ulang terlebih dahulu dikarenakan setelah data cleaning kemungkinan akan ada perubahan duplikasi data.
"""

# Memeriksa duplikasi data books
jumlah_duplikat = books.duplicated().sum()
print(f"Jumlah baris duplikat pada books: {jumlah_duplikat}")

# Memeriksa duplikasi data users
jumlah_duplikat = users.duplicated().sum()
print(f"Jumlah baris duplikat pada users: {jumlah_duplikat}")

# Memeriksa duplikasi data ratings
jumlah_duplikat = ratings.duplicated().sum()
print(f"Jumlah baris duplikat pada ratings: {jumlah_duplikat}")

"""Dapat dilihat bahwa terdapat 314 data duplikat pada books dan 2 data duplikat pada ratings yang kemudian perlu dihapus."""

books = books.drop_duplicates()
ratings = ratings.drop_duplicates()

"""## **Merging Dataset**

Tahapan persiapan data selanjutnya melibatkan penggabungan dataset (merging) dan pengambilan sampel (sampling). Proses penggabungan dilakukan dengan menggunakan fungsi `pd.merge()` sebanyak dua kali.
"""

dataset = pd.merge(books, ratings, on='ISBN', how='inner')
dataset = pd.merge(dataset, users, on='User-ID', how='inner')

"""DataFrame books dan ratings digabungkan berdasarkan kolom 'ISBN' dengan metode 'inner', menghasilkan DataFrame dataset yang berisi hanya baris dengan nilai 'ISBN' yang sama di kedua DataFrame. Kemudian, DataFrame dataset ini digabungkan kembali dengan DataFrame users berdasarkan kolom 'User-ID' juga menggunakan metode 'inner'

Setelah penggabungan, dilakukan pengambilan sampel (sampling) dengan menggunakan fungsi `dataset.sample(n=10000, random_state=42)` untuk memilih secara acak 10000 baris dari DataFrame dataset
"""

#Ambil sample 10000 baris agar tidak memperberat proses komputasi
dataset = dataset.sample(n=10000, random_state=42)

"""Berikut adalah hasil dari merging dan sampling."""

dataset.info()

dataset.head()

"""## **Persiapan Content-Based Filtering**

**Konversi Data dalam Bentuk List**

Langkah pertama adalah mengonversi data menjadi list, yang akan memudahkan kita dalam membuat dataframe baru atau mengolah data lebih lanjut. Dalam hal ini, kita akan mengonversi kolom-kolom seperti Book-Title, Book-Author, dan Publisher ke dalam list.
"""

# Mengonversi data series ‘Book-Title’ menjadi dalam bentuk list
book_title = dataset['Book-Title'].tolist()

# Mengonversi data series ‘Book-Author’ menjadi dalam bentuk list
book_author = dataset['Book-Author'].tolist()

# Mengonversi data series ‘Publisher’ menjadi dalam bentuk list
publisher = dataset['Publisher'].tolist()

# Mengonversi data series ‘ISBN’ menjadi dalam bentuk list
isbn = dataset['ISBN'].tolist()

# Print panjang list untuk memastikan data dikonversi dengan benar
print(len(book_title))
print(len(book_author))
print(len(publisher))
print(len(isbn))

"""**Membuat DataFrame Baru**

Selanjutnya, kita akan membuat dictionary untuk menggabungkan data book_title, book_author, dan publisher menjadi satu DataFrame baru yang siap digunakan untuk model sistem rekomendasi berbasis konten.
"""

# Membuat dictionary untuk data ‘book_title’, ‘book_author’, dan ‘publisher’
book_data = pd.DataFrame({
    'book_title': book_title,
    'book_author': book_author,
    'publisher': publisher
})

book_data.head()

"""**Menghapus Duplikasi DataFrame Baru**

Selanjutnya, dilakukan penghapusan duplikasi pada DataFrame book_data dengan fokus pada kolom 'book_title'. Sebanyak 12.553 baris duplikat teridentifikasi dan dihapus. Hal ini penting karena satu buku dapat memiliki beberapa entri akibat adanya rating berbeda pada dataset awal.
"""

jumlah_duplikat = book_data.duplicated().sum()
print(f"Jumlah baris duplikat: {jumlah_duplikat}")

book_data = book_data.drop_duplicates(subset=['book_title'])

"""**TF-IDF Vectorizer**

Pada langkah ini, kita akan menggunakan TF-IDF Vectorizer untuk mendapatkan representasi fitur penting dari setiap judul buku berdasarkan kategori atau fitur yang relevan. Di sini kita akan fokus pada kategori Book-Title dan Book-Author untuk membangun fitur.
"""

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer(ngram_range=(1, 2), min_df = 1, stop_words='english')

# Melakukan perhitungan idf pada data 'Book-Title'
tf.fit(book_data['book_title'])

# Menampilkan fitur nama dari hasil perhitungan tf-idf
tf.get_feature_names_out()

"""Setelah menghitung IDF, kita dapat melakukan transformasi untuk menghasilkan matriks TF-IDF yang menggambarkan hubungan antara setiap judul buku."""

# Melakukan transformasi data 'Book-Title' menjadi matriks tf-idf
tfidf_matrix = tf.fit_transform(book_data['book_title'])

# Melihat ukuran matriks tf-idf
print(tfidf_matrix.shape)

"""Untuk menghasilkan vektor tf-idf dalam bentuk matriks, gunakan fungsi todense()."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""**Mengubah Tipe Data Matriks TF-IDF**

Selanjutnya, matriks TF-IDF diubah tipe datanya menjadi float32 untuk mengoptimalkan penggunaan memori dan mempercepat proses komputasi tanpa mengurangi presisi signifikan.
"""

# Mengonversi matriks tf-idf ke tipe data float32
normalized_df = tfidf_matrix.astype(np.float32)

"""## **Persiapan Collaborative Filtering**"""

# Membaca dataset
df = dataset
df

"""**Penghapusan Kolom untuk Menyederhanakan Dataset**

Kolom `Year-Of-Publication`, `Publisher`, `Age`, `City`, `State`, dan `Country` dihapus untuk menyederhanakan data dan fokus pada fitur yang relevan, sehingga meningkatkan efisiensi dan akurasi model rekomendasi.
"""

df = df.drop(['Year-Of-Publication','Publisher','Age','City','State','Country'], axis=1)

"""**Encoding Data**

Selanjutnya, dilakukan encoding `User-ID` dan `ISBN` ke integer untuk memudahkan pemrosesan oleh algoritma machine learning.
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['User-ID'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

book_ids = df['ISBN'].unique().tolist()
print('list ISBN: ', book_ids)

# Melakukan proses encoding isbn
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
print('encoded ISBN : ', book_to_book_encoded)

# Melakukan proses encoding angka ke isbn
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}
print('encoded angka ke ISBN: ', book_encoded_to_book)

"""**Memetakan `User-ID` dan `ISBN` ke dataframe yang berkaitan**

Selanjutnya, dilakukan pemetaan 'User-ID' dan 'ISBN' ke kolom baru 'user' dan 'book' dalam DataFrame.  Kolom 'user' dibuat dengan memetakan setiap 'User-ID' asli ke representasi integer yang telah di-encode menggunakan kamus user_to_user_encoded, sementara kolom 'book' dihasilkan dengan memetakan 'ISBN' asli ke integer yang di-encode menggunakan kamus book_to_book_encoded
"""

# Mapping user_id ke dataframe user
df['user'] = df['User-ID'].map(user_to_user_encoded)

# Mapping isbn ke dataframe buku
df['book'] = df['ISBN'].map(book_to_book_encoded)

"""**Mengecek Jumlah User dan Jumlah Buku dan Mengubah Nilai Rating menjadi float**

Selanjutnya, jumlah pengguna (num_users) dihitung dari ID pengguna yang sudah di-encode, dan jumlah buku (num_books) dihitung dari ID buku yang sudah di-encode.

Kemudian, dilakukan pemeriksaan dan penentuan nilai minimum (min_rating) serta nilai maksimum (max_rating) dari kolom `Book-Rating`. Proses ini diawali dengan mengubah tipe data kolom `Book-Rating` menjadi float, kemudian nilai minimum dan maksimum dari seluruh rating yang ada diambil.
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_books = len(book_encoded_to_book)
print(num_books)

# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['Book-Rating'])

# Nilai maksimal rating
max_rating = max(df['Book-Rating'])

print('Number of User: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_books, min_rating, max_rating
))

"""**Membagi Data untuk Training dan Validasi**

Sebelum membagi data menjadi training dan validasi, seluruh data terlebih dahulu diacak agar distribusi data lebih merata dan tidak bias.
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""Kemudian, fitur input (user dan book ID) dan target (rating) dipisahkan. Rating dinormalisasi ke rentang 0-1 agar lebih stabil saat dipelajari model. Setelah itu, data dibagi menjadi 80% data pelatihan dan 20% data validasi untuk memastikan evaluasi model dilakukan secara adil terhadap data yang belum pernah dilihat sebelumnya."""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# **6. Modelling**

## **Content-Based Filtering**

Selanjutnya, kita akan menghitung derajat kesamaan (similarity degree) antara buku yang satu dengan buku lainnya dengan teknik cosine similarity. Di sini, kita menggunakan fungsi cosine_similarity dari library sklearn.
"""

# Menghitung cosine similarity antara setiap dokumen
cosine_sim = cosine_similarity(normalized_df, normalized_df)

# Menyusun matrix cosine similarity menjadi dataframe
cosine_sim_df = pd.DataFrame(cosine_sim, index=book_data['book_title'], columns=book_data['book_title'])

# Menampilkan hasil dari similarity matrix
cosine_sim_df.head()

"""Cosine similarity digunakan untuk menghitung kemiripan antar judul buku, menghasilkan matriks berukuran (10000, 10000). Matriks ini merepresentasikan tingkat kesamaan antar 10.000 judul. Karena ukurannya besar, hanya ditampilkan sebagian, yaitu 10 judul secara vertikal dan 5 secara horizontal. Data ini digunakan untuk merekomendasikan buku yang mirip dengan yang pernah dibaca pengguna."""

def book_recommendations(book_title, similarity_data=cosine_sim_df, items=book_data[['book_title', 'book_author', 'publisher']], k=5):
    """
    Rekomendasi Buku berdasarkan kemiripan dataframe

    Parameter:
    ---
    book_title : tipe data string (str)
                 Judul Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan buku sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung judul buku dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---

    Pada index ini, kita mengambil k buku dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """

    # Pastikan book_title ada di dalam similarity_data.columns
    if book_title not in similarity_data.columns:
        print(f"'{book_title}' tidak ditemukan di dalam data.")
        return None

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    index = similarity_data.loc[:, book_title].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-(k+2):-1]]  # Ambil k item teratas selain buku yang dicari

    # Drop nama_buku agar judul buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    # Menggabungkan rekomendasi buku dengan data tambahan seperti penulis atau penerbit
    recommended_books = pd.DataFrame(closest, columns=['book_title']).merge(items, on='book_title')

    return recommended_books.head(k)

"""Dengan menggunakan `argpartition`, diambil sejumlah nilai *similarity* tertinggi dari `cosine_sim_df`, lalu diurutkan dari yang paling mirip ke paling rendah dan disimpan dalam variabel `closest`. Judul buku yang menjadi input akan dihapus dari hasil agar tidak muncul dalam daftar rekomendasi yang diberikan.

Gunakan fungsi book_recommendation untuk mendapatkan rekomendasi berdasarkan judul buku
"""

book_recommendations("Harry Potter und der Stein der Weisen")

"""Berdasarkan output diatas, sistem berhasil merekomendasikan 5 judul buku teratas dengan kategori nama penulis (book_author) yaitu 'J.K. Rowling' dan memiliki judul yang mirip (masih dalam satu rangkaian series Harry Potter).

## **Collaborative Filtering**

**Membuat Kelas RecommenderNet**

Pada proses pelatihan, model menghitung tingkat kecocokan antara pengguna dan buku dengan memanfaatkan embedding. Data pengguna dan judul buku pertama-tama diubah menjadi embedding, lalu dilakukan operasi dot product antara kedua embedding tersebut. Selain itu, bias individual untuk pengguna dan buku turut ditambahkan. Hasil kecocokan ini kemudian diubah menjadi nilai antara 0 sampai 1 menggunakan fungsi aktivasi sigmoid.

Model ini dibangun sebagai sebuah kelas bernama `BookRecommenderNet` yang merupakan turunan dari kelas `Model` di Keras.
"""

import tensorflow as tf
class BookRecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size, dropout_rate=0.2, **kwargs):
        super(BookRecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_books = num_books
        self.embedding_size = embedding_size
        self.dropout_rate = dropout_rate

        # User embeddings
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)  # User bias

        # Book embeddings
        self.book_embedding = layers.Embedding(
            num_books,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.book_bias = layers.Embedding(num_books, 1)  # Book bias
        self.dropout = layers.Dropout(rate=dropout_rate)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])  # User embedding
        user_vector = self.dropout(user_vector)
        user_bias = self.user_bias(inputs[:, 0])  # User bias
        book_vector = self.book_embedding(inputs[:, 1])  # Book embedding
        book_vector = self.dropout(book_vector)
        book_bias = self.book_bias(inputs[:, 1])  # Book bias

        # Dot product antara vektor pengguna dan buku
        dot_user_book = tf.tensordot(user_vector, book_vector, 2)

        # Menambahkan bias pengguna dan bias buku
        x = dot_user_book + user_bias + book_bias

        return tf.nn.sigmoid(x)  # Aktivasi sigmoid untuk prediksi rating

"""Selanjutnya, lakukan proses compile terhadap model."""

# Iniisialisasi Model
model = BookRecommenderNet(num_users, num_books, 50)

# Compile Model
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini memanfaatkan Binary Crossentropy sebagai fungsi loss, menggunakan optimizer Adam (Adaptive Moment Estimation), dan mengukur performa dengan metrik root mean squared error (RMSE).

Lakukan pelatihan
"""

history = model.fit(
    x=x_train,  # Input data (User-ID, ISBN)
    y=y_train,  # Target data (ratings)
    batch_size=64,
    epochs=50,
    validation_data=(x_val, y_val)  # Validation data
)

"""**Mendapatkan Rekomendasi**

Setelah model dilatih, proses rekomendasi dimulai dengan memilih satu pengguna secara acak dari dataset. Dari pengguna ini, diidentifikasi buku-buku yang sudah diberi rating dan buku yang belum pernah dinilai.

Buku-buku yang belum dinilai di-encode sesuai dengan format embedding, kemudian dibuat array input gabungan antara user yang sudah di-encode dan buku-buku yang belum dinilai.
"""

# Memuat dataset Ratings.csv dan Book
book_df = books
df = pd.read_csv('Ratings.csv')

# Mengambil sampel acak dari user
user_id = df['User-ID'].sample(1).iloc[0]  # Mengambil 1 user secara acak, menggunakan bracket notation
books_rated_by_user = df[df['User-ID'] == user_id]  # Buku yang sudah dinilai oleh user tersebut, menggunakan bracket notation

# Mengambil buku yang belum dinilai oleh user tersebut
books_not_rated = book_df[~book_df['ISBN'].isin(books_rated_by_user['ISBN'].values)]['ISBN']


# Memastikan bahwa buku yang belum dinilai terdapat dalam set buku yang sudah terenkode (book_to_book_encoded)
books_not_rated = list(
    set(books_not_rated)
    .intersection(set(book_to_book_encoded.keys()))
)

# Meng-encode buku yang belum dinilai
books_not_rated = [[book_to_book_encoded.get(x)] for x in books_not_rated]

# Meng-encode user_id
user_encoder = user_to_user_encoded.get(user_id)

# Membuat array input untuk prediksi: User dan Buku yang Belum Dinilai
user_books_array = np.hstack(
    ([[user_encoder]] * len(books_not_rated), books_not_rated)
)

"""Model kemudian memprediksi rating untuk setiap buku yang belum pernah dinilai tersebut. Dari hasil prediksi ini, diambil 10 buku dengan prediksi rating tertinggi sebagai rekomendasi. Selain itu, untuk memberikan konteks preferensi pengguna, juga ditampilkan 5 buku dengan rating tertinggi yang sudah pernah dinilai oleh pengguna. Semua rekomendasi buku dan buku yang sudah dinilai ini ditampilkan lengkap dengan judul dan pengarangnya"""

# Melakukan prediksi rating untuk buku yang belum dinilai oleh pengguna
ratings = model.predict(user_books_array).flatten()

# Mengambil 10 buku dengan rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Mengonversi indeks rekomendasi menjadi ID buku yang sesuai
recommended_book_ids = [
    book_encoded_to_book.get(books_not_rated[x][0]) for x in top_ratings_indices
]

# Menampilkan rekomendasi untuk pengguna
print('Showing recommendations for user: {}'.format(user_id))
print('===' * 9)
print('Books with high ratings from user')
print('----' * 8)

# Menampilkan 5 buku yang telah diberi rating tertinggi oleh pengguna
top_books_user = (
    books_rated_by_user.sort_values(
        by='Book-Rating', # Sorting by 'Book-Rating' not 'rating'
        ascending=False
    )
    .head(5)
    ['ISBN'].values # Getting ISBN values not bookID values
)

# Mengambil data buku yang sudah dinilai oleh pengguna dan menampilkannya
book_df_rows = book_df[book_df['ISBN'].isin(top_books_user)]
for index, row in book_df_rows.iterrows(): # Iterating through DataFrame rows
    print(row['Book-Title'], ':', row['Book-Author']) # Accessing columns by name

print('----' * 8)
print('Top 10 book recommendations')
print('----' * 8)

# Menampilkan 10 buku rekomendasi teratas berdasarkan prediksi
recommended_books = book_df[book_df['ISBN'].isin(recommended_book_ids)]
for index, row in recommended_books.iterrows(): # Iterating through DataFrame rows
     print(row['Book-Title'], ':', row['Book-Author']) # Accessing columns by name

"""# **7. Evaluation**

## **Evaluasi Content-Based Filtering**

Ground truth dibentuk berdasarkan nilai cosine similarity, dengan ambang batas (threshold) sebesar 0.5. Jika similarity ≥ 0.5, maka dianggap mirip (1), jika tidak maka 0. Matriks ground truth dibuat menggunakan np.where() dan disajikan dalam bentuk DataFrame dengan indeks berupa book_title.
"""

# Menetapkan batas nilai kemiripan untuk klasifikasi biner (1 atau 0)
batas_threshold = 0.5

# Menghasilkan data ground truth berdasarkan batas nilai yang ditentukan
ground_truth = np.where(cosine_sim >= batas_threshold, 1, 0)

# Menampilkan sebagian nilai dari matriks ground truth dalam bentuk DataFrame
ground_truth_df = pd.DataFrame(
    ground_truth,
    index=book_data['book_title'],
    columns=book_data['book_title']
).sample(n=5, axis=1).sample(n=10, axis=0)

"""Matriks 2 dimensi diubah menjadi array 1 dimensi (flattened) agar dapat dibandingkan langsung elemen per elemen saat evaluasi."""

cosine_flat = cosine_sim.flatten()
truth_flat = ground_truth.flatten()

"""Untuk evaluasi model, digunakan metrik precision, recall, dan f1-score menggunakan precision_recall_fscore_support dari Scikit-learn. Matriks similarity dan ground truth dikonversi menjadi array 1 dimensi, lalu diklasifikasikan biner berdasarkan threshold 0.5. Evaluasi dilakukan pada seluruh data, dengan parameter average='binary' dan zero_division=1."""

# Prediksi: klasifikasi biner berdasarkan ambang batas similarity
prediksi = (cosine_flat >= batas_threshold).astype(int)

# Evaluasi model dengan precision, recall, dan f1-score
precision, recall, f1_score, _ = precision_recall_fscore_support(
    truth_flat, prediksi, average='binary', zero_division=1
)

# Tampilkan hasil evaluasi
print("=== Hasil Evaluasi Model Rekomendasi ===")
print(f"Precision : {precision:.2f}")
print(f"Recall    : {recall:.2f}")
print(f"F1-score  : {f1_score:.2f}")

"""Didapatkan hasil precision, recall, dan f1-score adalah 1.0

Nilai evaluasi ini mengindikasikan bahwa sistem rekomendasi bekerja sangat baik pada subset data yang diuji, menghasilkan rekomendasi yang sangat akurat dan komprehensif.

## **Evaluasi Collaborative Filtering**

Metrik yang digunakan dalam proyek ini untuk mengevaluasi performa model rekomendasi menggunakan Collaborative Filtering adalah Root Mean Squared Error (RMSE).
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Model Rekomendasi Buku - RMSE Selama Epoch')
plt.ylabel('Root Mean Squared Error (RMSE)')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""Grafik RMSE selama epoch menunjukkan performa model rekomendasi buku. RMSE pada data training turun dari sekitar 0.31 ke 0.19, menandakan model belajar dengan baik. Namun, RMSE pada data testing tetap stabil di sekitar 0.31, mengindikasikan sedikit overfitting dan keterbatasan generalisasi. Meski demikian, perbedaan nilai RMSE yang kecil menunjukkan model sudah cukup baik. Untuk perbaikan, dapat dilakukan regularisasi lebih kuat, penyesuaian dropout, atau menambah data pelatihan.

"""